---
title: "Dendrogyra_16S"
author: "J. Meyer"
date: "2/11/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries

```{r, echo=FALSE}
library(dada2)
library(ShortRead)
library(ggplot2)
library(phyloseq)
library(vegan)
library(knitr)
library(ALDEx2)
library(CoDaSeq)
library(zCompositions)
library(igraph)
library(car)
library(grDevices)
library(propr)
library(cowplot)
library(randomcoloR)
library(dplyr)
library(reshape2)
library(tibble)
library(exactRankTests)
library(nlme)
library(data.table)
library(Rmisc)
library(indicspecies)
library(viridis)
writeLines(capture.output(sessionInfo()), "sessionInfo.txt")
```

## Quality-filter the sequencing reads and create Amplicon Sequence Variant (ASV) tables with DADA2

Put unjoined R1 and R2 fastq files, with adaptors and primers previously removed with cutadapt into a directory for DADA2. Here, our forward and reverse fastq filenames have format: SAMPLENAME_R1_cut.fastq.gz and SAMPLENAME_R2_cut.fastq.gz

*****If you have samples from multiple sequencing runs, you need to determine the sequence variants for each run separately, then merge the ASV tables.
Here is the dada2 page on merging runs: https://benjjneb.github.io/dada2/bigdata_paired.html


```{r, echo=FALSE}
#### 1st sequencing run
path <- "cutadapt1"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "-"), `[`, 1)
# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# some of the forward reads ended with no reads after filtering. using the filtered table as a guide, remove those sequence files (F and R) from the cutadapt folder, delete the filtered folder, and rerun this block
write.table(out, "filtered1.txt",sep="\t",col.names=NA)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats1.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "seqtab1.rds") 
```


```{r, echo=FALSE}
#### 2nd sequencing run
path <- "cutadapt2"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "-"), `[`, 1)
# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# some of the forward reads ended with no reads after filtering. using the filtered table as a guide, remove those sequence files (F and R) from the cutadapt folder, delete the filtered folder, and rerun this block
write.table(out, "filtered2.txt",sep="\t",col.names=NA)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats2.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "seqtab2.rds")
```


```{r, echo=FALSE}
#### 3rd sequencing run
path <- "cutadapt3"
list.files(path)
fnFs <- sort(list.files(path, pattern="_R1_cut.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_cut.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "-"), `[`, 1)
# Perform filtering and trimming
filt_path <- file.path(path, "filtered") 
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
# some of the forward reads ended with no reads after filtering. using the filtered table as a guide, remove those sequence files (F and R) from the cutadapt folder, delete the filtered folder, and rerun this block
write.table(out, "filtered3.txt",sep="\t",col.names=NA)
# Learn the Error Rates, it TAKES TIME!
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
# Dereplicate the filtered fastq files
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names
names(derepRs) <- sample.names
# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
# Inspecting the dada-class object returned by dada:
dadaFs[[1]]
# Merge the denoised forward and reverse reads:
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled")
rownames(track) <- sample.names
head(track)
write.table(track, "dada_read_stats3.txt",sep="\t",col.names=NA)
saveRDS(seqtab, "seqtab3.rds")
```

Now that I have determined the ASV tables for all 3 sequencing runs, I can merge the ASV tables and remove chimera sequences.

```{r, echo=FALSE}
st1 <- readRDS("seqtab1.rds") 
st2 <- readRDS("seqtab2.rds") 
st3 <- readRDS("seqtab3.rds") 
st.all <- mergeSequenceTables(st1, st2, st3, repeats="sum") # You may get the message "Duplicated sample names detected in the sequence table row names." to let you know that there are duplicate names across samples - it is not an error, just a message. If you have run a sample on more than one sequencing run, the ASV counts will be added together.
#Remove chimeric sequences:
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(st.all)
# Combine read stats from 4 runs and add chimera summary - This does not combine duplicate rows from repeated samples.
stat1 <- read.table("dada_read_stats1.txt",sep="\t",header=TRUE, row.names=1)
stat2 <- read.table("dada_read_stats2.txt",sep="\t",header=TRUE, row.names=1)
stat3 <- read.table("dada_read_stats3.txt",sep="\t",header=TRUE, row.names=1)
stats.all<-bind_rows(stat1,stat2,stat3) 
write.table(stats.all, "dada_read_stats_all.txt",sep="\t",col.names=NA)
# Track reads through the pipeline
# As a final check of our progress, weâ€™ll look at the number of reads that made it through each step in the pipeline
rowSums(seqtab.nochim)
# need to write this out to add to dada read stats
# SAVE the non-chimeric sequence variant table SO YOU DON'T HAVE TO REPEAT ALL OF THE ABOVE STEPS
saveRDS(seqtab.nochim, file="dendro.rds")
# RELOAD THE SAVED INFO FROM HERE (if you have closed the project):
#seqtab.nochim <- readRDS("dendro.rds")
```

## Assign taxonomy in DADA2

Make sure the taxonomy reference database is in your working directory. Keep the database file gzipped. Adjust path name below. This step is very time consuming.

When taxonomy assignment is complete, we will use base R and phyloseq to clean up the taxonomy table. First, we will replace NAs and empty cells with the lowest taxonomy classification available. Second, we will use phyloseq to remove reads that are classified as Eukaryotes or unclassified at the domain level (ie, we are keeping only Bacteria and Archaea because that is what our primers target).

```{r, echo=FALSE}
taxa <- assignTaxonomy(seqtab.nochim, "silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)
# FIX the NAs in the taxa table
taxon <- as.data.frame(taxa,stringsAsFactors=FALSE)
taxon$Phylum[is.na(taxon$Phylum)] <- taxon$Kingdom[is.na(taxon$Phylum)]
taxon$Class[is.na(taxon$Class)] <- taxon$Phylum[is.na(taxon$Class)]
taxon$Order[is.na(taxon$Order)] <- taxon$Class[is.na(taxon$Order)]
taxon$Family[is.na(taxon$Family)] <- taxon$Order[is.na(taxon$Family)]
taxon$Genus[is.na(taxon$Genus)] <- taxon$Family[is.na(taxon$Genus)]
write.table(taxon,"silva_taxa_table.txt",sep="\t",col.names=NA)
write.table(seqtab.nochim, "silva_otu_table.txt",sep="\t",col.names=NA)
# Create phyloseq object from otu and taxonomy tables from dada2, along with the sample metadata.
otu <- read.table("silva_otu_table.txt",sep="\t",header=TRUE, row.names=1)
taxon <- read.table("silva_taxa_table.txt",sep="\t",header=TRUE,row.names=1)
samples<-read.table("metadata.txt",sep="\t",header=T,row.names=1)
OTU = otu_table(otu, taxa_are_rows=FALSE)
taxon<-as.matrix(taxon)
TAX = tax_table(taxon)
sampledata = sample_data(samples)
ps <- phyloseq(otu_table(otu, taxa_are_rows=FALSE), 
               sample_data(samples), 
               tax_table(taxon))
ps  #8188 taxa and 171 samples
# remove chloroplasts and mitochondria and Eukaryota
get_taxa_unique(ps, "Family") 
get_taxa_unique(ps, "Order") 
get_taxa_unique(ps, "Kingdom") 
ps <- subset_taxa(ps, Family !="Mitochondria")
ps <- subset_taxa(ps, Order !="Chloroplast")
ps <- subset_taxa(ps, Kingdom !="Eukaryota")
ps <- subset_taxa(ps, Kingdom !="NA")
get_taxa_unique(ps, "Family") 
get_taxa_unique(ps, "Order") 
get_taxa_unique(ps, "Kingdom")
ps #7559 taxa and 171 samples
# Now export cleaned otu and taxa tables from phyloseq for future reference
otu = as(otu_table(ps), "matrix")
taxon = as(tax_table(ps), "matrix")
metadata = as(sample_data(ps), "matrix")
write.table(otu,"silva_nochloronomito_otu_table.txt",sep="\t",col.names=NA)
write.table(taxon,"silva_nochloronomito_taxa_table.txt",sep="\t",col.names=NA)
# export ASV table as relative abundance
ps_ra<-transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
otu_ra = as(otu_table(ps_ra), "matrix")
write.table(otu_ra,"silva_nochloronomito_otu_table_RA.txt",sep="\t",col.names=NA)
```

Now, time to explore the data.

```{r, echo=FALSE}

ps #7559 taxa and 171 samples
get_taxa_unique(ps, "Order") #307
get_taxa_unique(ps, "Class") #130
ps5<-filter_taxa(ps, function(x) mean(x) >5, TRUE)
ntaxa(ps5) #148
ps10<-filter_taxa(ps, function(x) mean(x) >10, TRUE)
ntaxa(ps10) #74
get_taxa_unique(ps, "Genus") #1068
get_taxa_unique(ps5, "Genus") #88
get_taxa_unique(ps10, "Genus") #51

# Now export filtered otu and taxa tables from phyloseq for future reference
otu_ps5 = as(otu_table(ps5), "matrix")
taxon_ps5 = as(tax_table(ps5), "matrix")
metadata = as(sample_data(ps5), "matrix")
write.table(otu_ps5,"silva_nochloronomito_otu_table_ps5.txt",sep="\t",col.names=NA)
write.table(taxon_ps5,"silva_nochloronomito_taxa_table_ps5.txt",sep="\t",col.names=NA)
write.table(metadata,"metadata.txt",sep="\t",col.names=NA)
ps5_ra<-transform_sample_counts(ps5, function(OTU) OTU/sum(OTU))
otu_ps5_ra = as(otu_table(ps5_ra), "matrix")
write.table(otu_ps5_ra,"silva_nochloronomito_otu_table_ps5_RA.txt",sep="\t",col.names=NA)

otu_ps10 = as(otu_table(ps10), "matrix")
taxon_ps10 = as(tax_table(ps10), "matrix")
metadata = as(sample_data(ps10), "matrix")
write.table(otu_ps10,"silva_nochloronomito_otu_table_ps10.txt",sep="\t",col.names=NA)
write.table(taxon_ps10,"silva_nochloronomito_taxa_table_ps10.txt",sep="\t",col.names=NA)
write.table(metadata,"metadata.txt",sep="\t",col.names=NA)
ps10_ra<-transform_sample_counts(ps10, function(OTU) OTU/sum(OTU))
otu_ps10_ra = as(otu_table(ps10_ra), "matrix")
write.table(otu_ps10_ra,"silva_nochloronomito_otu_table_ps10_RA.txt",sep="\t",col.names=NA)
```

## Perform center-log-ratio transformation on ASVs and calculate Aitchison Distance and principal components


```{r, echo=FALSE}
# create a phyloseq object with the ASV table that has been filtered to remove low-abundance ASVs
otu <- read.table("silva_nochloronomito_otu_table_ps5.txt",sep="\t",header=TRUE, row.names=1)
taxon <- read.table("silva_nochloronomito_taxa_table_ps5.txt",sep="\t",header=TRUE,row.names=1)
samples<-read.table("metadata.txt",sep="\t",header=T,row.names=1)
OTU = otu_table(otu, taxa_are_rows=FALSE)
taxon<-as.matrix(taxon)
TAX = tax_table(taxon)
sampledata = sample_data(samples)
ps5 <- phyloseq(otu_table(otu, taxa_are_rows=FALSE), 
               sample_data(samples), 
               tax_table(taxon))
ps5 # 148 taxa, 171 samples

# First, replace 0 values with an estimate (because normalization is taking log, can't have 0)
# Also transposing here, need samples as rows
d.czm <- cmultRepl(t(otu), method="CZM", label=0)
# Perform the center-log-ratio (CLR) transformation 
d.clr <- codaSeq.clr(d.czm)
# transpose matrix of CLR transformed data for ordination and dendrogram
E.clr <- t(d.clr)
# plot compositional PCA biplot (perform a singular value decomposition)
d.pcx <- prcomp(E.clr)
# calculate percent variance explained for the axis labels
pc1 <- round(d.pcx$sdev[1]^2/sum(d.pcx$sdev^2),2)
pc2 <- round(d.pcx$sdev[2]^2/sum(d.pcx$sdev^2),2)
xlab <- paste("PC1: ", pc1, sep="")
ylab <- paste("PC2: ", pc2, sep="")
#biplot(d.pcx, cex=c(0.6,0.4), var.axes=F,scale=1, xlab=xlab, ylab=ylab)
summary(d.pcx)
str(d.pcx)
screeplot(d.pcx)
# replot PCA with ggplot2 (showing samples only)
df_out <- as.data.frame(d.pcx$x)
theme_set(theme_bw()+theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()))

pdf("PCA.pdf",bg = "white",width=8.5)
p<-ggplot(df_out,aes(x=PC1,y=PC2,colour=samples$Location))
p<-p+geom_point(size=3)+
  theme(axis.title = element_text(size=14))+
  theme(axis.text=element_text(size=12))+
  theme(legend.title = element_text(size=14))+
  theme(legend.text = element_text(size=12))+
  guides(fill = guide_legend(override.aes=list(shape=21)))
p + labs(x=xlab, y=ylab, colour="Location") + coord_fixed()
dev.off()

# set metadata as factors for anosim/permanova
reef<-as.character(samples$Reef)
location<-as.character(samples$Location)

# permanova/anosim between groups using Aitchison distance
dist.clr <- dist(E.clr)

perm<-adonis(dist.clr~location,as(sample_data(ps5),"data.frame"))
print(perm)

perm<-adonis(dist.clr~reef,as(sample_data(ps5),"data.frame"))
print(perm)

ano.reef <- anosim(dist.clr, reef, permutations=999)
plot(ano.reef)

ano.location <- anosim(dist.clr, location, permutations=999)
plot(ano.location)

```


```{r, echo=FALSE}

#### Making bar charts using ps10 (low abundance filtered)
ps10 #74 taxa and 171 samples
ps10_ra<-transform_sample_counts(ps10, function(OTU) OTU/sum(OTU))
ps10_ra
#figure out how many colors you need
get_taxa_unique(ps10_ra, "Order") #37
get_taxa_unique(ps10_ra, "Class") #25
get_taxa_unique(ps10_ra, "Family") #44
get_taxa_unique(ps10_ra, "Genus") #51

#Set your color palette.
n <- 25
palette <- distinctColorPalette(n)
#you can rerun the previous line to get a new selection of colors
# keep list of colors used in palette that is most appealing
sink("palette.txt")
print(palette)
sink()

pdf("BarChart_class_location.pdf", width=10, height=8)
p1=plot_bar(ps10_ra, fill="Class")+
  geom_bar(aes(fill=Class), stat="identity",position="stack") +
  theme(strip.text=element_text(face="bold"))+
  theme(axis.text.x=element_blank())+
  scale_fill_manual(values=palette)+
  theme(plot.title = element_text(face="italic"))+
  theme(legend.position = "bottom") +
  theme(axis.title.x = element_blank())+
  facet_wrap(.~Location, scales="free")
p1
dev.off()


n <- 44
palette <- distinctColorPalette(n)
#you can rerun the previous line to get a new selection of colors
# keep list of colors used in palette that is most appealing
sink("palette2.txt")
print(palette2)
sink()

pdf("BarChart_family_location.pdf", width=10, height=8)
p2=plot_bar(ps10_ra, fill="Family")+
  geom_bar(aes(fill=Family), stat="identity",position="stack") +
  theme(strip.text=element_text(face="bold"))+
  theme(axis.text.x=element_blank())+
  scale_fill_manual(values=palette2)+
  theme(plot.title = element_text(face="italic"))+
  theme(legend.position = "bottom") +
  theme(axis.title.x = element_blank())+
  facet_wrap(.~Location, scales="free")
p2
dev.off()

n <- 51
palette3 <- distinctColorPalette(n)
sink("palette3.txt")
print(palette3)
sink()

pdf("BarChart_genus_location.pdf", width=10, height=8)
p3=plot_bar(ps10_ra, fill="Genus")+
  geom_bar(aes(fill=Genus), stat="identity",position="stack") +
  theme(strip.text=element_text(face="bold"))+
  theme(axis.text.x=element_blank())+
  scale_fill_manual(values=palette3)+
  theme(plot.title = element_text(face="italic"))+
  theme(legend.position = "bottom") +
  theme(axis.title.x = element_blank())+
  facet_wrap(.~Location, scales="free")
p3
dev.off()

```

